import sys
from datetime import datetime, timedelta
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from pyspark.sql import SparkSession

# Initialize Spark + Glue Context
spark = SparkSession.builder \
    .config("spark.sql.catalog.glue_catalog", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.glue_catalog.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog") \
    .config("spark.sql.catalog.glue_catalog.warehouse", "s3://your-bucket/warehouse/") \
    .config("spark.sql.catalog.glue_catalog.io-impl", "org.apache.iceberg.aws.s3.S3FileIO") \
    .enableHiveSupport() \
    .getOrCreate()

glueContext = GlueContext(SparkContext.getOrCreate())

# Configuration
catalog_name = "glue_catalog"
db_name = "your_database"
table_name = "your_table"
full_table_name = f"{catalog_name}.{db_name}.{table_name}"

# Number of days to retain snapshots
expire_snapshots_older_than_days = 7

try:
    # ‚úÖ Step 1: Optimize the entire Iceberg table (includes hidden partitions)
    print(f"\nüì¶ Optimizing all partitions of table: {full_table_name}")
    spark.sql(f"""
        CALL {catalog_name}.system.optimize('{db_name}.{table_name}')
    """)
    print("‚úÖ Optimization complete.")

    # ‚úÖ Step 2: Expire old snapshots
    expiration_cutoff = datetime.utcnow() - timedelta(days=expire_snapshots_older_than_days)
    expiration_str = expiration_cutoff.strftime('%Y-%m-%d %H:%M:%S')

    print(f"\nüßπ Expiring snapshots older than: {expiration_str}")
    spark.sql(f"""
        CALL {catalog_name}.system.expire_snapshots(
            '{db_name}.{table_name}',
            TIMESTAMP '{expiration_str}'
        )
    """)
    print("‚úÖ Snapshot expiration complete.")

except Exception as e:
    print(f"‚ùå Job failed: {e}")
    sys.exit(1)