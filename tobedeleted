from pyspark.sql import SparkSession

# Initialize a Spark session with necessary configurations for AWS S3 access
spark = SparkSession.builder \
    .appName("S3 CSV Header Merge") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.access.key", "YOUR_ACCESS_KEY") \
    .config("spark.hadoop.fs.s3a.secret.key", "YOUR_SECRET_KEY") \
    .getOrCreate()

# Core parts of the expected column names (e.g., any expected prefix or keyword)
expected_core_names = ["changerequest", "cidetails"]

def contains_expected_prefix(column, expected_parts):
    """
    Check if any part of the expected header keywords exists in the column name.
    """
    return any(part in column.lower() for part in expected_parts)

def is_header(columns, expected_parts):
    """
    Check if the columns in the DataFrame likely represent headers based on expected keywords.
    """
    return all(contains_expected_prefix(col, expected_parts) for col in columns)

# S3 bucket path containing your CSV files
bucket_path = 's3a://your-bucket-name/path/to/csv/files/'

# List of CSV files in the bucket (Spark can handle this listing if configured correctly)
csv_files = spark.sparkContext.wholeTextFiles(bucket_path + "*.csv").keys().collect()

headers = None
header_file = None
non_header_files = []

# Identify the file with headers and separate the ones without headers
for file_path in csv_files:
    df_test = spark.read.format("csv").option("inferSchema", "true").load(file_path).limit(10)
    if is_header(df_test.columns, expected_core_names):
        headers = df_test.columns
        header_file = file_path
    else:
        non_header_files.append(file_path)

# Now read the file with headers correctly
df_with_header = spark.read.format("csv").option("header", "true").load(header_file)

# Read files without headers using the identified headers from the file with headers
dataframes = [df_with_header]
for file_path in non_header_files:
    df_no_header = spark.read.format("csv").option("header", "false").load(file_path).toDF(*headers)
    dataframes.append(df_no_header)

# Union all dataframes by column names
combined_df = dataframes[0]
for df in dataframes[1:]:
    combined_df = combined_df.unionByName(df)

# Transform data (example: filling missing values)
combined_df = combined_df.na.fill("Default Value")

# Load data (example: saving to a new combined CSV in S3)
output_path = 's3a://your-bucket-name/path/to/save/combined_csv.csv'
combined_df.write.format("csv").option("header", "true").save(output_path)

# Stop the Spark session
spark.stop()
